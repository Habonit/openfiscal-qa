{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 19:20:59.776\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[34m\u001b[1mPython version        : 3.11.11\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.778\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mPyTorch version       : 2.7.0+cu126\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.779\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mTransformers version  : 4.52.3\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mLangChain version     : 0.3.25\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mlangchain-huggingface version : 0.2.0\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1mlangchain-openai version       : 0.3.18\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.787\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m42\u001b[0m - \u001b[34m\u001b[1mlangchain-chroma version       : 0.2.4\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.788\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mOpenAI version        : 1.82.1\u001b[0m\n",
      "\u001b[32m2025-05-31 19:20:59.789\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mPandas version        : 2.2.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "import langchain\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import openai\n",
    "import torch\n",
    "\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "import shutil\n",
    "import os \n",
    "\n",
    "# langchain-huggingface 같은 플러그인 형태의 라이브러리를 알기 위한 함수입니다. \n",
    "def get_version(pkg):\n",
    "    try:\n",
    "        return version(pkg)\n",
    "    except PackageNotFoundError:\n",
    "        return \"N/A\"\n",
    "\n",
    "logger.debug(f\"Python version        : {platform.python_version()}\")\n",
    "logger.debug(f\"PyTorch version       : {torch.__version__}\")\n",
    "logger.debug(f\"Transformers version  : {transformers.__version__}\")\n",
    "logger.debug(f\"LangChain version     : {langchain.__version__}\")\n",
    "logger.debug(f\"langchain-huggingface version : {get_version('langchain-huggingface')}\")\n",
    "logger.debug(f\"langchain-openai version       : {get_version('langchain-openai')}\")\n",
    "logger.debug(f\"langchain-chroma version       : {get_version('langchain-chroma')}\")\n",
    "logger.debug(f\"OpenAI version        : {openai.__version__}\")\n",
    "logger.debug(f\"Pandas version        : {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env의 내용을 모두 환경 변수로 접근 가능하게 만듦\n",
    "load_dotenv()\n",
    "\n",
    "# .env의 정보를 대문자 변수로 저장합니다.\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "HF_API_KEY=os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "TRAIN_CSV=os.getenv(\"TRAIN_CSV\")\n",
    "TRAIN_SOURCE_DIR=os.getenv(\"TRAIN_SOURCE_DIR\")\n",
    "TEST_CSV=os.getenv(\"TEST_CSV\")\n",
    "TEST_SOURCE_DIR=os.getenv(\"TEST_SOURCE_DIR\")\n",
    "SAMPLE_CSV=os.getenv(\"SAMPLE_CSV\")\n",
    "SUBMISSION_PATH=os.getenv(\"SUBMISSION_PATH\")\n",
    "\n",
    "VECTORDB_BASE=os.getenv(\"VECTORDB_BASE\")\n",
    "\n",
    "CHUNK_SIZE=os.getenv(\"CHUNK_SIZE\")\n",
    "CHUNK_OVERLAP=os.getenv(\"CHUNK_OVERLAP\")\n",
    "\n",
    "# VALIDATION_MODE = true => openai 검증 모델 사용\n",
    "# VALIDATION_MODE = false => huggingface 모델 사용\n",
    "VALIDATION_MODE = os.getenv(\"VALIDATION_MODE\").lower() == \"true\"\n",
    "\n",
    "# SAMPLES = all => 마지막에 모든 샘플 추론\n",
    "# SAMPLES = 특정 숫자 => 그 수까지만 추론\n",
    "SAMPLES = os.getenv(\"SAMPLES\", \"all\")\n",
    "if SAMPLES.lower() == \"all\":\n",
    "    sample_limit = None \n",
    "else:\n",
    "    try:\n",
    "        sample_limit = int(SAMPLES)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid SAMPLES value: {SAMPLES}\")   \n",
    "    \n",
    "HUGGINGFACE_EMBEDDING_MODEL=os.getenv(\"HUGGINGFACE_EMBEDDING_MODEL\")\n",
    "OPENAI_EMBEDDING_MODEL=os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "HUGGINGFACE_LANGUAGE_MODEL=os.getenv(\"HUGGINGFACE_LANGUAGE_MODEL\")\n",
    "OPENAI_LANGUAGE_MODEL=os.getenv(\"OPENAI_LANGUAGE_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENUM으로 텍스트 데이터를 관리합니다.\n",
    "class PDFParseMethod(str, Enum):\n",
    "    UNSTRUCTURED = \"UnstructuredPdfLoader\"\n",
    "    PDFPLUMBER = \"PdfPlumberLoader\"\n",
    "    \n",
    "class EmbeddingModel(str, Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    HUGGINGFACE = \"huggingface\"\n",
    "    \n",
    "class RetrievalMethod(str, Enum):\n",
    "    SIMILARITY = \"similarity\"\n",
    "    MMR = \"mmr\"\n",
    "    BM25 = \"bm25\"\n",
    "    ENSEMBLE = \"ensemble\"\n",
    "    \n",
    "class LanguageModel(str, Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    HUGGINGFACE = \"huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 19:11:45.513\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/재정통계해설.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/국토교통부_소규모주택정비사업.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/고용노동부_조기재취업수당.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/국토교통부_민간임대(융자).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/2024년도 성과계획서(총괄편).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/보건복지부_노인일자리 및 사회활동지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/고용노동부_청년일자리창출지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.518\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/1-1 2024 주요 재정통계 1권.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/중소벤처기업부_창업사업화지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/보건복지부_생계급여.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/고용노동부_내일배움카드(일반).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/국토교통부_전세임대(융자).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.524\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/월간 나라재정 2023년 12월호.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/「FIS 이슈 & 포커스」 22-3호 《재정융자사업》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:11:45.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/train_source/2024 나라살림 예산개요.pdf\u001b[0m\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "\u001b[32m2025-05-31 19:13:10.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/보건복지부_부모급여(영아수당) 지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/국토교통부_행복주택출자.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/산업통상자원부_에너지바우처.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/「FIS 이슈&포커스」 22-2호 《재정성과관리제도》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/중소벤처기업부_혁신창업사업화자금(융자).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:10.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_pdf_path\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1m[_get_pdf_path] Found PDF: data/test_source/보건복지부_노인장기요양보험 사업운영.pdf\u001b[0m\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "# 제공된 데이터를 저장하는 클래스\n",
    "class DataProcessor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_csv=TRAIN_CSV,\n",
    "            train_source_dir=TRAIN_SOURCE_DIR,\n",
    "            test_csv=TEST_CSV,\n",
    "            test_source_dir=TEST_SOURCE_DIR,\n",
    "            sample_csv=SAMPLE_CSV\n",
    "        ):\n",
    "        self.train_df = pd.read_csv(train_csv)\n",
    "        self.test_df = pd.read_csv(test_csv)\n",
    "        self.submission_df = pd.read_csv(sample_csv)\n",
    "        self.train_source_dir = train_source_dir\n",
    "        self.test_source_dir = test_source_dir\n",
    "    \n",
    "    def get_train_df(self):\n",
    "        return self.train_df\n",
    "    def get_test_df(self):\n",
    "        return self.test_df\n",
    "    def get_submission_df(self):\n",
    "        return self.submission_df\n",
    "    def get_train_source_dir(self):\n",
    "        return self.train_source_dir\n",
    "    def get_test_source_dir(self):\n",
    "        return self.test_source_dir\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_pdf_path(directory: str) -> list:\n",
    "        dir_path = Path(directory)\n",
    "        if not dir_path.is_dir():\n",
    "            logger.error(f\"[_get_pdf_path] Invalid directory: {directory}\")\n",
    "            return []\n",
    "\n",
    "        pdf_paths = [p for p in dir_path.rglob(\"*.pdf\")]\n",
    "        for path in pdf_paths:\n",
    "            logger.debug(f\"[_get_pdf_path] Found PDF: {path}\")\n",
    "        return pdf_paths\n",
    "    \n",
    "    def convert_all_pdfs(\n",
    "            self,\n",
    "            file_path: str, \n",
    "            chunk_size: int = int(CHUNK_SIZE),\n",
    "            chunk_overlap: int = int(CHUNK_OVERLAP),\n",
    "            method: PDFParseMethod = PDFParseMethod.PDFPLUMBER,\n",
    "        ):\n",
    "        \n",
    "        pdf_path = DataProcessor._get_pdf_path(file_path)\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        if method == PDFParseMethod.UNSTRUCTURED:\n",
    "            output = {}\n",
    "            for path in pdf_path:\n",
    "                key = path.name\n",
    "                try:\n",
    "                    loader = UnstructuredPDFLoader(str(path), mode=\"elements\")\n",
    "                    docs = loader.load()\n",
    "                    chunks = splitter.split_documents(docs)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"[convert_all_pdfs: {method}] Failed to process {path}: {e}\")\n",
    "                    chunks = []\n",
    "                finally:\n",
    "                    output[key] = {\n",
    "                        \"chunks\" : chunks,\n",
    "                        \"path\" : str(path),\n",
    "                    }\n",
    "        \n",
    "        elif method == PDFParseMethod.PDFPLUMBER:\n",
    "            output = {}\n",
    "            for path in pdf_path:\n",
    "                key = path.name\n",
    "                try:\n",
    "                    loader = PDFPlumberLoader(str(path))  \n",
    "                    docs = loader.load()\n",
    "                    chunks = splitter.split_documents(docs)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"[convert_all_pdfs: {method}] Failed to process {path}: {e}\")\n",
    "                    chunks = []\n",
    "                finally:\n",
    "                    output[key] = {\n",
    "                        \"chunks\": chunks,\n",
    "                        \"path\": str(path),\n",
    "                    }\n",
    " \n",
    "        else:\n",
    "            msg = f\"Unsupported method: {method}\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "data_processor = DataProcessor()\n",
    "train_df = data_processor.get_train_df()\n",
    "test_df = data_processor.get_test_df()\n",
    "sample_df = data_processor.get_submission_df()\n",
    "\n",
    "# TODO: 추후엔 pdf로 파싱한 데이터를 깔끔하게 전처리하는 로직을 한 번 돌려야 합니다. \n",
    "train_pdf = data_processor.convert_all_pdfs(data_processor.get_train_source_dir())\n",
    "test_pdf = data_processor.convert_all_pdfs(data_processor.get_test_source_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크로마 기반 벡터스토어 저장\n",
    "class EmbeddingVectorStoreBuilder:\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_model: EmbeddingModel,\n",
    "        huggingface_model_name: Optional[str] = HUGGINGFACE_EMBEDDING_MODEL,\n",
    "        openai_model_name: Optional[str] = OPENAI_EMBEDDING_MODEL,\n",
    "        huggingface_api_key: Optional[str] = HF_API_KEY,\n",
    "        openai_api_key: Optional[str] = OPENAI_API_KEY,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.embedding_model_type = embedding_model\n",
    "        \n",
    "        if embedding_model == EmbeddingModel.OPENAI:\n",
    "            if not openai_api_key:\n",
    "                msg = \"OpenAI API key must be provided.\"\n",
    "                logger.error(ValueError(msg))\n",
    "                raise ValueError(msg)\n",
    "            self.embedding = OpenAIEmbeddings(\n",
    "                model=openai_model_name,\n",
    "                openai_api_key=openai_api_key\n",
    "            )\n",
    "                \n",
    "        elif embedding_model == EmbeddingModel.HUGGINGFACE:\n",
    "            model_kwargs = {'device': device}\n",
    "            if huggingface_api_key:\n",
    "                model_kwargs[\"token\"] = huggingface_api_key\n",
    "            encode_kwargs = {'normalize_embeddings': True}\n",
    "            \n",
    "            self.embedding = HuggingFaceEmbeddings(\n",
    "                model_name=huggingface_model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs    \n",
    "            )\n",
    "        else:\n",
    "            msg = f\"Unsupported embedding model: {embedding_model}\"\n",
    "            logger.error(ValueError(msg))\n",
    "            raise ValueError(msg)\n",
    "        \n",
    "    def build(self, chunks: List[Document], persist_directory: Optional[Union[Path, str]] = None) -> Chroma:\n",
    "        persist_directory = Path(persist_directory)\n",
    "        persist_directory.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"[build]: Building Chroma vector DB at {persist_directory}\")\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embedding,\n",
    "            persist_directory=str(persist_directory)\n",
    "        )\n",
    "        return vectordb\n",
    "\n",
    "# OPENAI = \"openai\"\n",
    "# HUGGINGFACE = \"huggingface\"\n",
    "if VALIDATION_MODE:\n",
    "    builder = EmbeddingVectorStoreBuilder(\n",
    "        embedding_model=EmbeddingModel.OPENAI\n",
    "    )\n",
    "else:\n",
    "    builder = EmbeddingVectorStoreBuilder(\n",
    "        embedding_model=EmbeddingModel.HUGGINGFACE\n",
    "    )\n",
    "\n",
    "base = Path(VECTORDB_BASE) / builder.embedding_model_type\n",
    "if base.exists():\n",
    "    shutil.rmtree(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 19:13:17.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/재정통계해설.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:21.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/국토교통부_소규모주택정비사업.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:23.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/고용노동부_조기재취업수당.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:24.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/국토교통부_민간임대(융자).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:24.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/2024년도 성과계획서(총괄편).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:31.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/보건복지부_노인일자리 및 사회활동지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:31.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/고용노동부_청년일자리창출지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:32.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/1-1 2024 주요 재정통계 1권.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:36.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/중소벤처기업부_창업사업화지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:37.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/보건복지부_생계급여.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:38.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/고용노동부_내일배움카드(일반).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:39.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/국토교통부_전세임대(융자).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:40.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:42.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/월간 나라재정 2023년 12월호.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:45.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/「FIS 이슈 & 포커스」 22-3호 《재정융자사업》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:47.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/2024 나라살림 예산개요.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:52.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:53.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:55.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/보건복지부_부모급여(영아수당) 지원.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:55.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/국토교통부_행복주택출자.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:56.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/산업통상자원부_에너지바우처.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:57.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/「FIS 이슈&포커스」 22-2호 《재정성과관리제도》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:58.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/중소벤처기업부_혁신창업사업화자금(융자).pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:13:59.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf\u001b[0m\n",
      "\u001b[32m2025-05-31 19:14:00.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m[build]: Building Chroma vector DB at chroma_db/EmbeddingModel.OPENAI/보건복지부_노인장기요양보험 사업운영.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# pdf 별 벡터 db 저장\n",
    "for key in train_pdf.keys():\n",
    "    doc_dir = base / key\n",
    "    _ = builder.build(\n",
    "        chunks = train_pdf[key]['chunks'],\n",
    "        persist_directory=doc_dir\n",
    "    )\n",
    "    \n",
    "for key in test_pdf.keys():\n",
    "    doc_dir = base / key\n",
    "    _ = builder.build(\n",
    "        chunks = test_pdf[key]['chunks'],\n",
    "        persist_directory=doc_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: Union[OpenAIEmbeddings, HuggingFaceEmbeddings],\n",
    "        base_directory: Union[str, Path]\n",
    "    ):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.base_directory = Path(base_directory)\n",
    "        \n",
    "    def _load_vectordb(self, db_key: str) -> Chroma:\n",
    "        vectordb_path = self.base_directory / Path(db_key)\n",
    "        if not vectordb_path.exists():\n",
    "            msg = f\"Vector DB not found ad {vectordb_path}\"\n",
    "            logger.error(msg)\n",
    "            raise FileExistsError(msg)\n",
    "        \n",
    "        vectordb = Chroma(\n",
    "            persist_directory=str(vectordb_path),\n",
    "            embedding_function=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        return vectordb\n",
    "    \n",
    "    def _load_docs(self, vectordb: Chroma) -> List[Document]:\n",
    "        data = vectordb.get(include=[\"documents\", \"metadatas\"])\n",
    "        return [\n",
    "            Document(page_content=doc, metadata=meta)\n",
    "                for doc, meta in zip(data[\"documents\"], data[\"metadatas\"])\n",
    "        ]\n",
    "    \n",
    "    # TODO: 각종 인자를 .env로 관리해야 합니다. \n",
    "    def run(\n",
    "        self,\n",
    "        query: str,\n",
    "        db_key: str,\n",
    "        method: RetrievalMethod = RetrievalMethod.SIMILARITY,\n",
    "        k: int = 5,\n",
    "        alpha: float = 0.5\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        vectordb = self._load_vectordb(db_key)\n",
    "        \n",
    "        if method == RetrievalMethod.SIMILARITY:\n",
    "            return vectordb.similarity_search(query, k=k)\n",
    "        \n",
    "        elif method == RetrievalMethod.MMR:\n",
    "            return vectordb.max_marginal_relevance_search(query, k=k)\n",
    "        \n",
    "        elif method == RetrievalMethod.BM25:\n",
    "            docs = self._load_docs(vectordb)\n",
    "            retriever = BM25Retriever.from_documents(docs)\n",
    "            retriever.k = k\n",
    "            return retriever.invoke(query)\n",
    "        \n",
    "        elif method == RetrievalMethod.ENSEMBLE:\n",
    "            docs = self._load_docs(vectordb)\n",
    "            bm25 = BM25Retriever.from_documents(docs)\n",
    "            bm25.k = k\n",
    "            dense = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "            ensemble = EnsembleRetriever(retrievers=[bm25, dense], weights=[1-alpha, alpha])\n",
    "            return ensemble.invoke(query)\n",
    "        \n",
    "        else:\n",
    "            msg = f\"Unsupported method: {method}\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        \n",
    "retriever = RetrievalExecutor(\n",
    "    embedding_model=builder.embedding,\n",
    "    base_directory=base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [01:03<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: 간헐적으로 병목이 걸리는데 그 원인을 아직 파악하지 못했습니다.\n",
    "qa_pairs = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"[Retrieving] Document Retrieving\"):\n",
    "    question = row['Question']\n",
    "    key = Path(row['Source_path']).name\n",
    "    results = retriever.run(\n",
    "        query=question,\n",
    "        db_key=key,\n",
    "        method=RetrievalMethod.ENSEMBLE,\n",
    "    )\n",
    "    context_parts = [f\"### START-{i} ###{doc.page_content}### END-{i} ###\" for i, doc in enumerate(results)]\n",
    "    context = \"\".join(context_parts)\n",
    "    qa_pairs.append({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OpenAI Inference] model: gpt-4o-mini:   0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OpenAI Inference] model: gpt-4o-mini: 100%|██████████| 98/98 [04:15<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "class ChatResponder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: LanguageModel,\n",
    "        huggingface_model_name: Optional[str] = HUGGINGFACE_LANGUAGE_MODEL,\n",
    "        openai_model_name: Optional[str] = OPENAI_LANGUAGE_MODEL,\n",
    "        huggingface_api_key: Optional[str] = HF_API_KEY,\n",
    "        openai_api_key: Optional[str] = OPENAI_API_KEY,\n",
    "        system_prompt: str = \"당신은 정답을 친절하게 알려주는 비서입니다.\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"        \n",
    "    ):\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.system_prompt = system_prompt\n",
    "        \n",
    "        if model_type == LanguageModel.OPENAI:\n",
    "            self.model = openai_model_name\n",
    "            self.client = OpenAI(api_key=openai_api_key)\n",
    "            \n",
    "        elif model_type == LanguageModel.HUGGINGFACE:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                huggingface_model_name,\n",
    "                token=huggingface_api_key\n",
    "            )\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                huggingface_model_name,\n",
    "                token=huggingface_api_key,\n",
    "                trust_remote_code=True\n",
    "            ).to(device)\n",
    "\n",
    "            \n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=0 if device == \"cuda\" else -1,\n",
    "            )\n",
    "        else:\n",
    "            msg = f\"Unsupported model type: {model_type}\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "    \n",
    "    # TODO: prompt_template과 system_prompt는 .txt로 가지고 있어야 합니다.\n",
    "    # TODO: 각종 인자를 .env로 관리해야 합니다. \n",
    "    # TODO: batch_size 사용할 것인지 결정해야합니다.\n",
    "    def run(\n",
    "        self, \n",
    "        qa_pairs: List[Dict[str, str]], \n",
    "        prompt_template: str=\"아래 질문에 대하여 문맥에서 답을 찾아 {max_new_tokens}자 이내로 대답하세요. 질문: {question}, 문맥: {context}\", \n",
    "        max_new_tokens: int=256, \n",
    "        do_sample: bool=True, \n",
    "        temperature: float=0.7, \n",
    "        batch_size: int=2,\n",
    "        deliminator: str=\"##########\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        output = []\n",
    "        if self.model_type == LanguageModel.OPENAI:\n",
    "            for pair in tqdm(qa_pairs, desc=f\"[OpenAI Inference] model: {self.model}\"):\n",
    "                prompt = prompt_template.format(\n",
    "                    question=pair['question'],\n",
    "                    context=pair['context'],\n",
    "                    max_new_tokens=max_new_tokens\n",
    "                )\n",
    "                prompt = prompt + deliminator\n",
    "            \n",
    "                response = self.client.responses.create(\n",
    "                    model=self.model,\n",
    "                    input=[\n",
    "                        {\"role\": \"developer\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "                output_text = response.output_text.strip()\n",
    "                input_tokens = response.usage.input_tokens\n",
    "                output_tokens = response.usage.output_tokens\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"input_text\" : {\n",
    "                            \"system_prompt\" : self.system_prompt,\n",
    "                            \"question\" : pair['question'],\n",
    "                            \"context\" : pair['context'],\n",
    "                            \"prompt_template\" : prompt_template,\n",
    "                        },\n",
    "                        \"output_text\": output_text,\n",
    "                        \"input_tokens\": input_tokens,\n",
    "                        \"output_tokens\": output_tokens\n",
    "                    }\n",
    "                )            \n",
    "            \n",
    "        elif self.model_type == LanguageModel.HUGGINGFACE:\n",
    "            prompts = [\n",
    "                self.system_prompt + \n",
    "                \"\\n\" + \n",
    "                prompt_template.format(\n",
    "                    question=pair['question'],\n",
    "                    context=pair['context'],\n",
    "                    max_new_tokens=max_new_tokens\n",
    "                ) + \n",
    "                deliminator\n",
    "                    for pair in qa_pairs\n",
    "            ]\n",
    "            logger.info(f\"[HuggingFace Inference Start] model: {self.model}\")\n",
    "            results = self.pipeline(\n",
    "                prompts, \n",
    "                max_new_tokens=max_new_tokens, \n",
    "                do_sample=do_sample, \n",
    "                temperature=temperature,\n",
    "                # batch_size=batch_size\n",
    "            )\n",
    "            logger.info(f\"[HuggingFace Inference End] model: {self.model}\")\n",
    "            generated_texts = [r[0][\"generated_text\"] for r in results]\n",
    "            output_texts = [text.split(deliminator)[-1].strip() for text in generated_texts]\n",
    "            input_tokens_lst = [len(self.tokenizer.encode(prompt)) for prompt in prompts]\n",
    "            output_tokens_lst = [len(self.tokenizer.encode(output)) for output in output_texts]\n",
    "                    \n",
    "            for pair, output_text, input_tokens, output_tokens in zip(qa_pairs, output_texts, input_tokens_lst, output_tokens_lst):\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"input_text\" : {\n",
    "                            \"system_prompt\" : self.system_prompt,\n",
    "                            \"question\" : pair['question'],\n",
    "                            \"context\" : pair['context'],\n",
    "                            \"prompt_template\" : prompt_template\n",
    "                        },\n",
    "                        \"output_text\": output_text,\n",
    "                        \"input_tokens\": input_tokens,\n",
    "                        \"output_tokens\": output_tokens,\n",
    "                    }\n",
    "                )  \n",
    "\n",
    "        return output\n",
    "    \n",
    "if VALIDATION_MODE:\n",
    "    responder = ChatResponder(\n",
    "        model_type=LanguageModel.OPENAI,\n",
    "    )\n",
    "else:\n",
    "    responder = ChatResponder(\n",
    "        model_type=LanguageModel.HUGGINGFACE,\n",
    "    )\n",
    "\n",
    "outputs = responder.run(qa_pairs[:sample_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[range(len(outputs)), \"Answer\"] = [item['output_text'] for item in outputs]\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "submission_path = Path(SUBMISSION_PATH)\n",
    "ext = submission_path.suffix\n",
    "submission_path = Path(str(submission_path.with_suffix(\"\"))+f\"_{VALIDATION_MODE}_{timestamp}\"+ext)\n",
    "submission_dir = submission_path.parent\n",
    "if not submission_dir.exists():\n",
    "    submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "     \n",
    "sample_df.to_csv(submission_path, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
